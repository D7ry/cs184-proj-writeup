# CS 184: Computer Graphics and Imaging, Spring 2023
# Project 3-1: Path Tracer
## Website URL: todo

## Overview

In this project, we built a working path tracer that renders 3D images by tracing light paths through BVH trees. Recursively tracing light paths allows us to render the scene with global illumination.

## Part 1: Ray Generation and Scene Intersection

### Walk through the ray generation and primitive intersection parts of the rendering pipeline.

Ray generation is one of the first steps in the rendering pipeline. To render the world space onto the camera, we generate rays casting from the camera space to the world space through transformation. The rays are generated by sampling the camera's field of view and the camera's position. To implement ray generation, we place a camera sensor in the world space, whose 2D coordinate in the camera's world space FOV corresponds to the camera space. The normalized difference between the camera point and the sensor is thus the ray direction, with the ray origin being the camera position.

Generated rays are then cast into the world space. For rays that fail to intersect with any primitives, we simply ignore them(on the rendered image they will default to the background color). Rays that intersect with primitives will then proceed in the pipeline, where we calculate the radiance of the intersection point by either directly sampling the light source or by tracing the light path recursively.

### Explain the triangle intersection algorithm you implemented in your own words.

The triangle intersection implementation uses the Moller-Trumbore algorithm. The algorithm works as follows:

1. Calculating the relative edge vectors of the triangle
```cpp
Vector3D e1 = p2 - p1;
Vector3D e2 = p3 - p1;
```
2. Calculating the determinant of the matrix formed by the edge vectors and the ray direction
```cpp
Vector3D s1 = Vector3D::cross(r.d, e2);
float det = Vector3D::dot(s1, e1);
```
3. If the determinant is 0, the ray is parallel to the triangle and there is no intersection
```cpp
if (det == 0) return false;
```
4. We calculate the distance from the ray origin to the first vertex of the triangle
```cpp
float f = 1 / det;
Vector3D s = r.o - p1;
```
5. Then we obtain the barycentric coordinates`b1, b2` of the intersection point
```cpp
...
b1 = f * dot(s, s1); // b1 must be between 0 and 1
b2 = f * dot(r.d, s2); // b2 must be between 0 and 1
...
```
6. Finally, we obtain the time of intersection `t`
```cpp
t = f * dot(e2, s2);
```
Note `t` must be witing the range of the ray's `min_t` and `max_t`. If `t` provides a tighter bound than the current `max_t`, we update the `max_t` to `t` and return `true`, updating the intersection information as well, when the pointer is not null.

### Show images with normal shading for a few small .dae files.
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/q1_banana.png" align="middle" width="400px"/>
        <figcaption>banana.dae</figcaption>
      </td>
      <td>
        <img src="images/q1_cow.png" align="middle" width="400px"/>
        <figcaption>cow.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/q1_sphere.png" align="middle" width="400px"/>
        <figcaption>sphere.dae</figcaption>
      </td>
      <td>
        <img src="images/q1_gems.png" align="middle" width="400px"/>
        <figcaption>cbgems.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

## Part 2: Bounding Volume Hierarchy 

### Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.

Out BVH algorithm is a recursive divide algorithm that works on a sorted list. The algorithm works as follows:

1. The caller of `construct_bvh()` passes in a list of primitives sorted by the x coordinate of their centroids. Ties are broken by the y coordinate, then the z coordinate.

2. `construct_bvh` first look at the distance between the `start` and `end` iterators passed to it. If the distance is less than the maximum number of primitives per leaf, we instantiate a leaf node with start and end iterators pointing to the `start` and `end` arguments passed in, returning the node. This is the base case of the recursion where we return a leaf node.

3. Otherwise, we instantiate an internal node. The node's lhs and rhs are assigned to the result of recursively calling `construct_bvh` on the left and right halves of the original list, separated by the average of the x coordinate of the centroids in the list. 

4. When "splitting" the original list, we make sure that we don't assign all elements to either the left or right half, which leads to infinite recursion, since none is reduced. If this happens, we instead use the median of the centroids as the splitting point. This is best illustrated by the following code:
```cpp
double avg_centroid; // avg of centroid on PRED_AXIS axis
.... // calculate avg_centroid
auto comp = [](const Primitive* p, double val) -> bool {
  return p->get_bbox().centroid().PRED_AXIS < val;
};

auto lBegin = a_start;
auto lEnd = std::lower_bound(a_start, a_end, avg_centroid, comp); // binary search to find end of lhs

if (lEnd == lBegin || lEnd == a_end) { // all the primitives on one side
  // in this case, split the lhs/rhs based on the median; since the vec is sorted, simply splitting the vec in 2 will do.
  lEnd = lBegin + size_t(distance(a_start, a_end) / 2);
}
auto rBegin = lEnd;
auto rEnd = a_end;
```

5. After getting the result of recursion, we expand the bounding box of the internal node to include the bounding boxes of its children.

6. Finally, we return the internal node.

### Extra credit portion

Out recursive algorithm works on a sorted vector with 2 pointers. It doesn't create any new vectors on the stack when doing recursive calls, nor does it create any new vectors on the heap to store information for the leaf nodes. In addition, it performs an elegant binary search when looking for the splitting point of the list through `std::lower_bound`, instead of an addition for loop for linear search(ugh). Furthermore, we can altogether eliminate the need for looping through the list by using the median of the centroids of the list, which simply invovles splitting the list in 2 equal halves. 

In conclusion, our BVH algorithm saves both space and time through a 2-pointer approach on a single vector, and a binary search to find the splitting point, with minimum overhead to sort the original list(std::sort is extremely efficient).

### Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/q2_blob.png" align="middle" width="400px"/>
        <figcaption>blob.dae</figcaption>
      </td>
      <td>
        <img src="images/q2_wall-e.png" align="middle" width="400px"/>
        <figcaption>wall-e.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/q2_lucy.png" align="middle" width="400px"/>
        <figcaption>lucy.dae</figcaption>
      </td>
      <td>
        <img src="images/q2_dragon.png" align="middle" width="400px"/>
        <figcaption>dragon.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

### Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.

The following table compares the rendering times of moderately complex scenes with and without BVH, in seconds:

| Scene | Without BVH | With BVH |
| :---: | :---------: | :------: |
|bunny.dae|75.2498s|0.2415s|
|cow.dae|17.9637s|0.2178s|
|lucy.dae|135.7630s|0.2836s|
|dragon.dae|83.4564s|0.5573s|
|cube.dae|0.1046s|0.0879s|

Image rendering time done with the help of BVH acceleration is significantly faster than without BVH. The difference is especially noticeable when the amount of primitives in the scene is large. This is because BVH allows a binary search to be performed on the primitives with O(log(n)) runtime, compared to the O(n) runtime without BVH. The rendering of `cube.dae`, however, takes less time without BVH than with; this is because the scene is so simple that the BVH construction overhead is not worth the time saved in the traversal.

## Part 3: Direct Illumination 

### Walk through both implementations of the direct lighting function.

#### Hemispheric Lighting

The hemispheric lighting implementation samples random points on the hemisphere centered at the intersection point between the ray and the primitive, whose normal is used as the hemisphere's axis: 
```cpp
Vector3D wi_sample hemisphereSampler->get_sample();
```
Additional rays are then cast from the intersection point to the hemisphere points. If the ray intersects with any primitive, we add the contribution of the light to the color of the intersection point, using the Monte Carlo integration method:
```cpp
if (bvh->intersect(r, &i_isect)) {
  CGL::Vector3D lighting = i_isect.bsdf->get_emission() * isect.bsdf->f(w_out, wi_sample) * wi_sample.z;
  L_out += lighting;
  // wi_sample.z is the cosine
}
```
Once we sample `num_samples` rays, we divide the accumulated lighting by `num_samples` to get the average lighting; in addition, we multiply the lighting by `2*PI` as the probability of sampling a point on the hemisphere is `1/(2*PI)`. We delay this multiplication to the end to save performance.

#### Importance Lighting

Instead of casting random rays from the intersection, importance lighting impl samples light sources in the existing scene, casting rays from the light source to the intersection point. Similarly, the light is then added to the color of the intersection point using the Monte Carlo integration method:

The following steps demonstrate a single pass of the importance lighting algorithm, performed on a single light source:

First, determine how many samples to perform on the light source. If the light source is a delta light, we don't need to sample it multiple times, as the light is only emitted from a single point.
```cpp
int sample_rep = light->is_delta_light() ? 1 : ns_area_light;
```

Then, sample `samlpe_rep` amount of rays and accumulate the lighting:
```cpp
Vector3D light_sample_avg = Vector3D(0.0);
for (int i = 0; i < sample_rep; i++) {
  Vector3D sample_light = light->sample_L(hit_p, &wi, &distToLight, &pdf);
  Ray r = Ray(hit_p, wi, distToLight - EPS_F);
  r.min_t = EPS_F;
  Intersection sample_intersect;
}
```

Note that here, the light source contributes to the intersection point only when no other primitives is in between them:

```cpp
if (!bvh->intersect(r, &sample_intersect)) {					
  light_sample_avg +=  // your fav monte carlo integration
    (isect.bsdf->f(w_out, w2o * wi)
    * sample_light
    * (w2o * wi).z)
      / pdf;
}
light_sample_avg /= sample_rep;
```

Finally, we average out the lighting from all light sources
```cpp
L_out /= scene->lights.size();
```

### Show some images rendered with both implementations of the direct lighting function.
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/q3_bunny_hemisphere_sampling.png" align="middle" width="400px"/>
        <figcaption>bunny.dae</figcaption>
      </td>
      <td>
        <img src="images/q3_bunny_importance_sampling.png" align="middle" width="400px"/>
        <figcaption>bunny.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/q3_spheres_hemisphere_lighting.png" align="middle" width="400px"/>
        <figcaption>CbSpheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="images/q3_spheres_importance_sampling.png" align="middle" width="400px"/>
        <figcaption>CbSpheres_lambertian.dae</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

### Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/q3_bunny_lightraycomp_1.png" align="middle" width="400px"/>
        <figcaption>1 Light Ray (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/q3_bunny_lightraycomp_4.png" align="middle" width="400px"/>
        <figcaption>4 Light Rays (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/q3_bunny_lightraycomp_16.png" align="middle" width="400px"/>
        <figcaption>16 Light Rays (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/q3_bunny_lightraycomp_64.png" align="middle" width="400px"/>
        <figcaption>64 Light Rays (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>

The noise level in soft shadows is significantly more noticeable at lower light ray count, and has been greatly reduced at higher light ray counts. This is because the areas supposed to be in soft shadow are much less likely to be in direct contact with the light rays cast from a point sampled from area lights. 

For example, when we only sample 1 light ray from the whole area light, chances are that the light ray is going to be occluded by the bunny. Even if the light ray hits the soft shadow area, the contribution of light from this one hit will account for the entire lighting of the hit point, leading to a soft shadow area that is much more noisy and not as smooth.

### Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.

Visually, lighting sampling produces much less noise under the same sampling configuration. 

## Part 4: Global Illumination

### Walk through your implementation of the indirect lighting function.

### Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>example1.dae</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>example2.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

### Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (example1.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>

### For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>

### Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>

## Part 5: Adaptive Sampling

### Explain adaptive sampling. Walk through your implementation of the adaptive sampling.

### Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.

<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/q5_bunny.png" align="middle" width="400px"/>
        <figcaption>Rendered image (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/q5_bunny_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/q5_sphere.png" align="middle" width="400px"/>
        <figcaption>Rendered image (example2.dae)</figcaption>
      </td>
      <td>
        <img src="images/q5_sphere_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (example2.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>